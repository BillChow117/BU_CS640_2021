{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wxSTIwOBJ66"
      },
      "source": [
        "# CS640 Assignment 4: Markov Decision Process\n",
        "\n",
        "In this assignment, you are asked to implement value iteration and policy iteration. We provide a script of skeleton code.\n",
        "\n",
        "Your tasks are the following.\n",
        "1. Implement the algorithms following the instruction.\n",
        "2. Run experiments and produce results.\n",
        "\n",
        "Do **not** modify the existing code, especially the variable names and function headers.\n",
        "\n",
        "## Submission\n",
        "Everything you need to complete for this assignment is in this notebook. Once you finish, please save this file as PDF and submit it via Gradescope. Make sure the outputs are saved when you create the PDF file!\n",
        "\n",
        "## Collaboration\n",
        "You must complete this assignment independently, but feel free to ask questions on Piazza. In particular, any questions that reveal your answers must be made private."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRDSg6QQCdB3"
      },
      "source": [
        "**Packages**\n",
        "\n",
        "The package(s) imported in the following block should be sufficient for this assignment, but you are free to add more if necessary. However, keep in mind that you **should not** import and use any MDP package. If you have concern about an addition package, please contact us via Piazza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SN6vCCz9BIHT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "np.random.seed(4) # for reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWTSOsluJrhY"
      },
      "source": [
        "**Examples for testing**\n",
        "\n",
        "The following block contains two examples used to test your code. You can create more for debugging, but please add it to a different block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Is4WhTckKCUC"
      },
      "outputs": [],
      "source": [
        "# a small MDP\n",
        "states = [0, 1, 2]\n",
        "actions = [0, 1] # 0 : stay, 1 : jump\n",
        "jump_probabilities = np.matrix([[0.1, 0.2, 0.7],\n",
        "                                [0.5, 0, 0.5],\n",
        "                                [0.6, 0.4, 0]])\n",
        "for i in range(len(states)):\n",
        "    jump_probabilities[i, :] /= jump_probabilities[i, :].sum()\n",
        "\n",
        "rewards_stay = np.array([0, 8, 5])\n",
        "rewards_jump = np.matrix([[-5, 5, 7],\n",
        "                          [2, -4, 0],\n",
        "                          [-3, 3, -3]])\n",
        "\n",
        "T = np.zeros((len(states), len(actions), len(states)))\n",
        "R = np.zeros((len(states), len(actions), len(states)))\n",
        "for s in states:\n",
        "    T[s, 0, s], R[s, 0, s] = 1, rewards_stay[s]\n",
        "    T[s, 1, :], R[s, 1, :] = jump_probabilities[s, :], rewards_jump[s, :]\n",
        "\n",
        "example_1 = (states, actions, T, R)\n",
        "\n",
        "# a larger MDP\n",
        "states = [0, 1, 2, 3, 4, 5, 6, 7]\n",
        "actions = [0, 1, 2, 3, 4]\n",
        "T = np.zeros((len(states), len(actions), len(states)))\n",
        "R = np.zeros((len(states), len(actions), len(states)))\n",
        "for a in actions:\n",
        "    T[:, a, :] = np.random.uniform(0, 10, (len(states), len(states)))\n",
        "    for s in states:\n",
        "        T[s, a, :] /= T[s, a, :].sum()\n",
        "    R[:, a, :] = np.random.uniform(-10, 10, (len(states), len(states)))\n",
        "\n",
        "example_2 = (states, actions, T, R)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAt8Laq5EfP6"
      },
      "source": [
        "**Value iteration**\n",
        "\n",
        "Implement value iteration by finishing the following function, and then run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0esiHIyOBEXb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example MDP 1\n",
            "Step 0\n",
            "state values: [0. 0. 0.]\n",
            "\n",
            "Step 1\n",
            "state values: [5.4 8.  5. ]\n",
            "\n",
            "Step 2\n",
            "state values: [5.94 8.8  5.5 ]\n",
            "\n",
            "Step 3\n",
            "state values: [5.994 8.88  5.55 ]\n",
            "\n",
            "Step 4\n",
            "state values: [5.9994 8.888  5.555 ]\n",
            "\n",
            "Optimal policy: [1 0 0]\n",
            "\n",
            "\n",
            "Example MDP 2\n",
            "Step 0\n",
            "state values: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "Step 1\n",
            "state values: [2.23688505 2.67355205 2.18175138 4.3596377  3.41342719 2.97145478\n",
            " 2.60531101 4.61040891]\n",
            "\n",
            "Step 2\n",
            "state values: [2.46057355 2.94090725 2.39992652 4.79560147 3.75476991 3.26860026\n",
            " 2.86584211 5.0714498 ]\n",
            "\n",
            "Step 3\n",
            "state values: [2.4829424  2.96764277 2.42174403 4.83919785 3.78890418 3.2983148\n",
            " 2.89189522 5.11755389]\n",
            "\n",
            "Optimal policy: [0 2 0 3 3 3 2 3]\n"
          ]
        }
      ],
      "source": [
        "def value_iteration(states, actions, T, R, gamma = 0.1, tolerance = 1e-2, max_steps = 100):\n",
        "    Vs = [] # all state values\n",
        "    Vs.append(np.zeros(len(states))) # initial state values\n",
        "    steps, convergent = 0, False\n",
        "    Q_values = np.zeros((len(states),len(actions)))\n",
        "    while not convergent:\n",
        "        ########################################################################\n",
        "        # TO DO: compute state values, and append it to the list Vs\n",
        "        # V_(k+1) = max_a sum_(next state s') T[s,a,s'] * (R[s,a,s'] + gamma * V-k(s))\n",
        "        V_next = np.zeros(len(states))\n",
        "        \n",
        "        for s in states:\n",
        "            V_next[s] = -sys.maxsize\n",
        "            for a in actions:\n",
        "                Q_value = 0\n",
        "                for s_ in states:\n",
        "                    Q_value += T[s,a,s_] * (R[s,a,s_] + gamma * Vs[-1][s])\n",
        "                V_next[s] = max(V_next[s],Q_value)\n",
        "                Q_values[s,a] = Q_value\n",
        "        Vs.append(V_next)\n",
        "        ############################ End of your code ##########################\n",
        "        steps += 1\n",
        "        convergent = np.linalg.norm(Vs[-1] - Vs[-2]) < tolerance or steps >= max_steps\n",
        "    ########################################################################\n",
        "    # TO DO: extract policy and name it \"policy\" to return\n",
        "    # Vs should be optimal\n",
        "    # the corresponding policy should also be the optimal one\n",
        "    policy = np.argmax(Q_values,axis=1)\n",
        "    ############################ End of your code ##########################\n",
        "    return Vs, policy, steps\n",
        "\n",
        "print(\"Example MDP 1\")\n",
        "states, actions, T, R = example_1\n",
        "gamma, tolerance, max_steps = 0.1, 1e-2, 100\n",
        "Vs, policy, steps = value_iteration(states, actions, T, R, gamma, tolerance, max_steps)\n",
        "for i in range(steps):\n",
        "    print(\"Step \" + str(i))\n",
        "    print(\"state values: \" + str(Vs[i]))\n",
        "    print()\n",
        "print(\"Optimal policy: \" + str(policy))\n",
        "print()\n",
        "print()\n",
        "print(\"Example MDP 2\")\n",
        "states, actions, T, R = example_2\n",
        "gamma, tolerance, max_steps = 0.1, 1e-2, 100\n",
        "Vs, policy, steps = value_iteration(states, actions, T, R, gamma, tolerance, max_steps)\n",
        "for i in range(steps):\n",
        "    print(\"Step \" + str(i))\n",
        "    print(\"state values: \" + str(Vs[i]))\n",
        "    print()\n",
        "print(\"Optimal policy: \" + str(policy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX5GMKMiHeGl"
      },
      "source": [
        "**Policy iteration**\n",
        "\n",
        "Implement policy iteration by finishing the following function, and then run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "uJXwe674Hbv3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example MDP 1\n",
            "Step 0\n",
            "state values: [0. 0. 0.]\n",
            "policy: [0 1 1]\n",
            "\n",
            "Step 1\n",
            "state values: [ 0.   1.  -0.6]\n",
            "policy: [1 0 0]\n",
            "\n",
            "\n",
            "Example MDP 2\n",
            "Step 0\n",
            "state values: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "policy: [3 2 1 4 3 3 4 0]\n",
            "\n",
            "Step 1\n",
            "state values: [ 1.79546043  2.67355205 -0.08665637 -4.92536024  3.41342719  2.97145478\n",
            " -1.69624246  2.48967841]\n",
            "policy: [0 2 0 3 3 3 2 3]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def policy_iteration(states, actions, T, R, gamma = 0.1, tolerance = 1e-2, max_steps = 100):\n",
        "    policy_list = [] # all policies explored\n",
        "    initial_policy = np.array([np.random.choice(actions) for s in states]) # random policy\n",
        "    policy_list.append(initial_policy)\n",
        "    Vs = [] # all state values\n",
        "    Vs = [np.zeros(len(states))] # initial state values\n",
        "    steps, convergent = 0, False\n",
        "    while not convergent:\n",
        "        ########################################################################\n",
        "        # TO DO:\n",
        "        # 1. Evaluate the current policy, and append the state values to the list Vs\n",
        "        # V[policy_i][k+1][s] = sum_(s_) T[s,policy_i[s],s_] * ( R[s,policy_i[s],s_ + gamma * V[policy_i][k][s_] )\n",
        "        V_next = np.zeros(len(states))\n",
        "\n",
        "        for s in states:\n",
        "            tmp = 0\n",
        "            for s_ in states:\n",
        "                tmp += T[s,policy_list[-1][s],s_] * ( R[s,policy_list[-1][s],s_] + gamma * Vs[-1][s_] )\n",
        "            V_next[s] = tmp\n",
        "        Vs.append(V_next)\n",
        "        # 2. Extract the new policy, and append the new policy to the list policy_list\n",
        "        # policy_list[i+1][s] = argmax_(a) sum_(s_) T[s,a,s_] * ( R[s,a,s_] + gamma * Vs[s_] )\n",
        "        policy_new = np.array([np.random.choice(actions) for s in states])\n",
        "        for s in states:\n",
        "            new_tmp = np.zeros((len(actions)))\n",
        "            for a in actions:\n",
        "                \n",
        "                sum = 0\n",
        "                for s_ in states:\n",
        "                    sum += T[s,a,s_] * ( R[s,a,s_] + gamma * Vs[-1][s_] )\n",
        "                new_tmp[a] = sum\n",
        "            policy_new[s] = np.argmax(new_tmp)\n",
        "        policy_list.append(policy_new)\n",
        "        ############################ End of your code ##########################\n",
        "        steps += 1\n",
        "        convergent = (policy_list[-1] == policy_list[-2]).all() or steps >= max_steps\n",
        "    return Vs, policy_list, steps\n",
        "\n",
        "print(\"Example MDP 1\")\n",
        "states, actions, T, R = example_1\n",
        "gamma, tolerance, max_steps = 0.1, 1e-2, 100\n",
        "Vs, policy_list, steps = policy_iteration(states, actions, T, R, gamma, tolerance, max_steps)\n",
        "for i in range(steps):\n",
        "    print(\"Step \" + str(i))\n",
        "    print(\"state values: \" + str(Vs[i]))\n",
        "    print(\"policy: \" + str(policy_list[i]))\n",
        "    print()\n",
        "print()\n",
        "print(\"Example MDP 2\")\n",
        "states, actions, T, R = example_2\n",
        "gamma, tolerance, max_steps = 0.1, 1e-2, 100\n",
        "Vs, policy_list, steps = policy_iteration(states, actions, T, R, gamma, tolerance, max_steps)\n",
        "for i in range(steps):\n",
        "    print(\"Step \" + str(i))\n",
        "    print(\"state values: \" + str(Vs[i]))\n",
        "    print(\"policy: \" + str(policy_list[i]))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyOUrPejJUL0"
      },
      "source": [
        "**More testing**\n",
        "\n",
        "The following block tests both of your implementations. Simply run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "uhadnld8JcYG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numbers of steps in value iteration: [4, 4, 5, 4, 5, 5, 5, 4, 4, 4, 5, 5, 4, 5, 5, 4, 5, 4, 5, 5]\n",
            "Numbers of steps in policy iteration: [2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2]\n"
          ]
        }
      ],
      "source": [
        "steps_list_vi, steps_list_pi = [], []\n",
        "for i in range(20):\n",
        "    states = [j for j in range(np.random.randint(5, 30))]\n",
        "    actions = [j for j in range(np.random.randint(2, states[-1]))]\n",
        "    T = np.zeros((len(states), len(actions), len(states)))\n",
        "    R = np.zeros((len(states), len(actions), len(states)))\n",
        "    for a in actions:\n",
        "        T[:, a, :] = np.random.uniform(0, 10, (len(states), len(states)))\n",
        "        for s in states:\n",
        "            T[s, a, :] /= T[s, a, :].sum()\n",
        "        R[:, a, :] = np.random.uniform(-10, 10, (len(states), len(states)))\n",
        "    Vs, policy, steps_v = value_iteration(states, actions, T, R)\n",
        "    Vs, policy_list, steps_p = policy_iteration(states, actions, T, R)\n",
        "    steps_list_vi.append(steps_v)\n",
        "    steps_list_pi.append(steps_p)\n",
        "print(\"Numbers of steps in value iteration: \" + str(steps_list_vi))\n",
        "print(\"Numbers of steps in policy iteration: \" + str(steps_list_pi))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "CS640Fall2021 MDP Homework.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "0f75978233c496711d8b6fbaac206609b5c6c1b124626415f694fad520d3d3bd"
    },
    "kernelspec": {
      "display_name": "Python 3.8.11 64-bit ('cs640': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
